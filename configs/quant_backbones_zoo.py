# Backbone architectures for quantization experimentation


"""
The neck must be added to stages, the "independent" (or lets change to "dag?") must be added to block names, in order to quantize the neck. Else everything is void.
In every part, the first convlayer is unaffected now. smart?
"""

REGISTERED_BACKBONE_VERSIONS = {
    # FP32 non-qunatized baseline has name '0' - not anymore! Now sending 0 gives quantize all.
    # baselines
    'FP32_baseline': {
        'stages': [],
        'block_names': [],
        'spare_bottlenecks': True,
        'spare_attention_qkv': True,
        'spare_attention_scaling': True,
        'spare_attention_projection': True,
    },
    'INT8_baseline': {
        'stages': ["stage0", "stage1", "stage2", "stage3", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    # FAMILY 1
    '1a': {
        'stages': ["stage0", "stage1", "stage2", "stage3"],
        'block_names': ["res", "fmb", "fmb", "mb"],
        'spare_bottlenecks': True,
    },
    '1b': {
        'stages': ["stage0"],
        'block_names': ["res", "fmb", "fmb", "mb"],
        'spare_bottlenecks': True,
    },
    '1c': {
        'stages': ["stage1"],
        'block_names': ["res", "fmb", "fmb", "mb"],
        'spare_bottlenecks': True,
    },
    '1d': {
        'stages': ["stage2"],
        'block_names': ["res", "fmb" "fmb", "mb"],
        'spare_bottlenecks': True,
    },
    '1e': {
        'stages': ["stage3"],
        'block_names': ["res", "fmb", "fmb", "mb"],
        'spare_bottlenecks': True,
    },
    # FAMILY 2 - to test attention stages
    '2a': { # " Stage 4 and 5,"
        'stages': ["stage4", "stage5"],
        'block_names': ["att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '2b': {
        'stages': ["stage4", "stage5"],
        'block_names': ["att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': True,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '2c': {
        'stages': ["stage4", "stage5"],
        'block_names': ["att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': True,
        'spare_attention_projection': False,
    },
    '2d': {
        'stages': ["stage4", "stage5"],
        'block_names': ["att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': True,
    },
    '2e': {
        'stages': ["stage4", "stage5"],
        'block_names': ["att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': True,
        'spare_attention_scaling': True,
        'spare_attention_projection': True,
    },
    # FAMILY 3 - to test the limits of XL1 - Lift one stage back to FP at a time. The model 3_q_all_convs is the baseline
    '3_q_all_but_stage0': {
        'stages': ["stage1", "stage2", "stage3", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False, 
    },
    '3_q_all_but_stage1': {
        'stages': ["stage0", "stage2", "stage3", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '3_q_all_but_stage2': {
        'stages': ["stage0", "stage1", "stage3", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '3_q_all_but_stage3': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '3_q_all_but_stage4': {
        'stages': ["stage0", "stage1", "stage2", "stage3", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '3_q_all_but_stage5': {
        'stages': ["stage0", "stage1", "stage2", "stage3", "stage4", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '3_q_all_but_bottlenecks': {
        'stages': ["stage0", "stage1", "stage2", "stage3", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '3_q_all_but_qkv': {
        'stages': ["stage0", "stage1", "stage2", "stage3", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': True,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    # Family 4 - investigating saving stage 3 together with various things, here one at a time
    '4_q_all_but_stage3_bottlenecks': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '4_q_all_but_stage3_qkv': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': True,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '4_q_all_but_stage3_scaling': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': True,
        'spare_attention_projection': False,
    },
    '4_q_all_but_stage3_projection': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': True,
    },
    # Family 4 part 2 - investigating saving stage3 + bottlenecks + parts of attention. Bias: Bottlenecks are the most important
    '4_q_all_but_stage3_bottleneck_qkv': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': True,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '4_q_all_but_stage3_bottleneck_scaling': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': True,
        'spare_attention_projection': False,
    },
    '4_q_all_but_stage3_bottleneck_projection': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': True,
    },
    '4_q_all_but_stage3_bottleneck_qkv_scaling_projection': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': True,
        'spare_attention_qkv': True,
        'spare_attention_scaling': True,
        'spare_attention_projection': True,
    },
    # Family 5 - abalation of block types instead of stages. Based on quantizing everything and then recover slective block types one at a time
    '5_q_all_but_ResBlocks': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "mb", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '5_q_all_but_MBConvs': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "fmb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '5_q_all_but_FusedMBConvs': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '5_q_all_but_Attention': { #should give same results as quantizing all but saving stage 4 + 5 - do check that it is the case!
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["independent", "res", "mb", "fmb"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    # Family 6 - the inverse of 5 - abalation of block types instead of stages, but start from non-quantized and quantize selectively.
    '5_q_only_ResBlocks': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["res"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '5_q_only_MBConvs': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["mb"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '5_q_only_FusedMBConvs': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["fmb"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
    '5_q_only_Attention': {
        'stages': ["stage0", "stage1", "stage2", "stage4", "stage5", "neck"],
        'block_names': ["att", "att@3", "att@5"],
        'spare_bottlenecks': False,
        'spare_attention_qkv': False,
        'spare_attention_scaling': False,
        'spare_attention_projection': False,
    },
}